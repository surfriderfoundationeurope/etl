{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surfrider ETL script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blob Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "from azure.storage.blob import BlobClient\n",
    "\n",
    "\n",
    "def blobInContainer(connection_s,container_n):\n",
    "    ''' \n",
    "    blobContainer create a name_list of blobs within container\n",
    "    Input: params are storage conn string & container name (no full url)\n",
    "    Output: the list of blobs objects within given container\n",
    "    '''\n",
    "    try:\n",
    "        campaign_container = ContainerClient.from_connection_string(conn_str=connection_s, container_name=container_n)\n",
    "        blob_list = campaign_container.list_blobs()\n",
    "        blob_names_list = []\n",
    "        for blob in blob_list:\n",
    "            blob_names_list.append(blob.name)\n",
    "        return blob_names_list\n",
    "    except:\n",
    "        print(\"The container you are trying to list blob from probably does not exist.\")\n",
    "        print(\"Early exit of ETL process as container probably does not exist.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "def blobInfos(connection_s,container_n,blob_n):\n",
    "    ''' \n",
    "    blobInfos provides basic information about a blob object\n",
    "    Input: params are storage conn string, container name and blob_name only (no full url)\n",
    "    Output: None, print only\n",
    "    '''\n",
    "    try:\n",
    "        blob_video = BlobClient.from_connection_string(conn_str=connection_s,container_name=container_n, blob_name=blob_n)\n",
    "        blob_video_url = blob_video.url\n",
    "        blob_video_prop = blob_video.get_blob_properties()\n",
    "        blob_video_prop_keys = blob_video_prop.keys()\n",
    "        print(\"Blob name:\",blob_n)\n",
    "        print(\"Blob URL:\",blob_video_url)\n",
    "        #print(\"blob properties:\", blob_video_prop)\n",
    "        #print(\"blob properties keys:\", blob_video_prop_keys)\n",
    "    except: \n",
    "        print(\"The blob you are trying to get info from probably does not exist\")\n",
    "\n",
    "\n",
    "def downloadBlob(blobclient):\n",
    "    ''' \n",
    "    downloadBlob from Azure to local file system\n",
    "    Input: parameter is a blob client object from azure storage sdk\n",
    "    Output: output is the path of the downloaded blob\n",
    "    '''\n",
    "    try:\n",
    "        with open(\"/tmp/\"+blobclient.blob_name, \"wb\") as my_blob_dl:\n",
    "            blob_data = blobclient.download_blob()\n",
    "            blob_data.readinto(my_blob_dl)\n",
    "        print(\"Blob %s downloaded\" %blobclient.blob_name)\n",
    "        print(\"Blob path: /tmp/%s\" %blobclient.blob_name)\n",
    "        path = \"/tmp/\"+blobclient.blob_name\n",
    "        return path\n",
    "    except:\n",
    "        print(\"The blob you are trying to download probably does not exist within container\")\n",
    "        print(\"Early exit of ETL process\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(video_name):\n",
    "    '''\n",
    "    getPrediction sends POST request to an AI inference service, delegated to bash script subprocess\n",
    "    Input: the name of a video which is expected to be dowloaded in local /tmp before\n",
    "    Output: the prediction made by AI: a json-like format data but as a list\n",
    "    '''\n",
    "    print(\"Sending video to AI for Trash prediction\")\n",
    "    curl_request_script = ['./curl_request_param.sh',video_name]\n",
    "    output = []\n",
    "    request_answer = subprocess.Popen(curl_request_script, stdout=subprocess.PIPE)\n",
    "    i = 0\n",
    "    for line in request_answer.stdout:\n",
    "        print(line)\n",
    "        output.append(line)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "def AIready(url):\n",
    "    '''\n",
    "    AIready function evaluate whether AI inference service is available\n",
    "    Input: takes the url of the AI service to evaluate availability\n",
    "    Output: returns ready status, a boolean status\n",
    "    '''\n",
    "    ready = False\n",
    "    try:\n",
    "        AI_request = requests.get(url)\n",
    "        print(\"HTTP Status Code: \",AI_request.status_code)\n",
    "        if AI_request.status_code == 200:\n",
    "            print(\"AI inference service is available\")\n",
    "            ready = True\n",
    "            return ready\n",
    "        else:\n",
    "            print(\"HTTP Status Code: \",AI_request.status_code)\n",
    "            print(\"AI server is responding but there might be an issue\")\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(\"AI not found, an error has occured\")\n",
    "        return ready\n",
    "\n",
    "\n",
    "def getPrediction(video_path):\n",
    "    '''\n",
    "    getPrediction sends POST request to an AI inference service, delegated to bash script subprocess\n",
    "    Input: the name of a video which is expected to be dowloaded in local /tmp before\n",
    "    Output: the prediction made by AI: a json-like format data but as a list\n",
    "    '''\n",
    "    files = {'file': (video_path, open(video_path, 'rb'), 'application/octet-stream')}\n",
    "    response = requests.post('http://aiapisurfrider.northeurope.cloudapp.azure.com:5000', files=files)\n",
    "    if not response.ok:\n",
    "        logger.error(f'Request to AI failed wih reason {response.reason}.')\n",
    "    output = [response._content]\n",
    "    return output\n",
    "\n",
    "\n",
    "def jsonPrediction(pred):\n",
    "    ''' \n",
    "    jsonPrediction cast a prediction from getPrediction function\n",
    "    Input: pred, the string result of the previous getPrediction function\n",
    "    Output: json_prediction, a dictionnary built from a subset of pred string\n",
    "    '''\n",
    "    string_prediction = str(pred[0])[2:-3] #removing 2 x first and 3 last characters of pred\n",
    "    json_prediction = json.loads(string_prediction)\n",
    "    return json_prediction\n",
    "\n",
    "\n",
    "def getTrashLabel(frame_2_box):\n",
    "    ''' \n",
    "    getTrashLabel return label from a frame_to_box\n",
    "    Input: a frame_2_box dictionnary from jsonPrediction\n",
    "    Output: the value of predicted label\n",
    "    '''\n",
    "    return frame_2_box['label']\n",
    "\n",
    "\n",
    "def mapLabel2TrashIdPG(label):\n",
    "    '''\n",
    "    mapLabel2TrashIdPG function is a different mapping between a predicted label by AI and TrashId as defined within TrashType table\n",
    "    Input: a label predicted by AI\n",
    "    Output: a TrashId as defined in TrashType table\n",
    "    '''\n",
    "    switcher = { \n",
    "        \"others\":\"1\", #\"autre dechet\" in PG Data Model mapped to IA \"others\" label\n",
    "        \"dechet agricole\":\"2\",\n",
    "        \"bottles\":\"3\", #\"bouteille boisson\" in PG Data Model mapped to IA \"bottles\" label\n",
    "        \"fragments\":\"4\",#\"industriel ou construction in PG Data Model mapped to IA \"fragments\" label\n",
    "        \"peche et chasse\":\"5\",\n",
    "        \"emballage alimentaire\":\"6\",\n",
    "        \"objet vie courante\":\"7\",\n",
    "        \"autres dechets +10\":\"8\"\n",
    "    } \n",
    "    return switcher.get(label, \"nothing\")\n",
    "\n",
    "\n",
    "def mapLabel2TrashIdSQL(label):\n",
    "    ''' \n",
    "    NOTICE: this switcher function is DEPRECATED as it initially standed for SQL Trash Table scheme\n",
    "    mapLabelTrashId is a switch that converts label to TrashId\n",
    "    Input: label that comes from getTrashLabel from jsonPrediction dictionnary \n",
    "    Output: a TrashId, which is meaningful with respect to Trash_Type table in PostGre\n",
    "    '''\n",
    "    switcher = { \n",
    "    \"Fishing or Hunting\":\"89B44BAA-69AA-4109-891A-128E012E7E07\",\n",
    "    \"Food Packaging\":\"185FEFA2-EEF2-47A8-873E-26032A4BB3C3\",\n",
    "    \"Unknown\":\"BB4DEA69-218A-40CC-A000-2AE17C37152C\",\n",
    "    \"Industrial or Construction Debris\":\"2A863E38-E5D0-455F-87CE-2B75DA29F59A\",\n",
    "    \"fragments\":\"ED401B92-DC24-44C0-A52A-34CE831092BF\",\n",
    "    \"Agricultural Waste\":\"36B2AFEB-7A7C-44B5-A790-5E5C73BA144D\",\n",
    "    \"others\":\"4BEC18FC-BC48-45B7-AFDA-6BA96BD80921\",\n",
    "    \"Common Household Items\":\"C68E90CF-6E65-4474-BC60-72E1C8513F55\",\n",
    "    \"plastic\":\"6961D0DB-928C-419E-9985-98EEEAF552C7\",\n",
    "    \"bottles\":\"9780940B-D06C-4AAB-8003-AB914981E87A\",\n",
    "    \"Drinking Bottles\":\"BCF549A8-AECD-4BC9-B9B8-B94A8F3758D5\",\n",
    "    \"Unknown10\":\"BC7BB564-BE04-4B4B-9913-FF69780B93A6\"\n",
    "    } \n",
    "    return switcher.get(label, \"nothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "video = '28022020_Boudigau_4.MP4'\n",
    "os.system(f'python /tmp/gopro2gpx/gopro2gpx.py -s -vvv /tmp/{video} /tmp/{video}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse GPX file\n",
    "import os\n",
    "import gpxpy\n",
    "import gpxpy.gpx\n",
    "import json\n",
    "import subprocess\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from shapely.geometry import Point\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from tqdm import tqdm\n",
    "\n",
    "def goproToGPX(video_name):\n",
    "    '''\n",
    "    goproToGPX function extracts GPX file from raw GoPro video\n",
    "    GPX extraction is delegated to shell scripts that calls gopro2gpx python helper\n",
    "    Input: the name of a video locally available within /tmp\n",
    "    Output: the path of the GPX generated file\n",
    "    '''\n",
    "    gopro2gpx_script = ['./gopro2gpx_param.sh', video_name]\n",
    "    result = subprocess.Popen(gopro2gpx_script, stdout=subprocess.PIPE)\n",
    "    output = []\n",
    "    i = 0\n",
    "    for line in result.stdout:\n",
    "        print(line)\n",
    "        output.append(line)\n",
    "    path= '/tmp/'+video_name+'.gpx'\n",
    "    return path\n",
    "\n",
    "\n",
    "def goproToGPX(video_path):\n",
    "    result = os.system(f'python /tmp/gopro2gpx/gopro2gpx.py -s -vvv {video_path} {video_path}')\n",
    "    path = video_path + '.gpx'\n",
    "    return path\n",
    "\n",
    "\n",
    "def gpsPointList(gpxdata):\n",
    "    ''' \n",
    "    gpsPointList function extract gps points from gpx file\n",
    "    Input: gpxdata is a gpxpy object that returns data from a parsed gpx file\n",
    "    Output: gpsPointList return a list of dictionnary points with Time, Lat, Long, Elev\n",
    "    '''\n",
    "\n",
    "    point_list = []\n",
    "    for track in gpxdata.tracks:\n",
    "        for segment in track.segments: \n",
    "            for point in segment.points:\n",
    "                point_info = {'Time':point.time,'Latitude':point.latitude,'Longitude':point.longitude,'Elevation':point.elevation}\n",
    "                point_list.append(point_info)\n",
    "    return point_list\n",
    "\n",
    "\n",
    "def getMediaInfo(mediafile):\n",
    "    '''\n",
    "    getMediaInfo function extract metadata info about a media file, using mediainfo shell command\n",
    "    Input: a media file like a video\n",
    "    Output: the metadata about the media\n",
    "    '''\n",
    "    cmd = \"mediainfo --Output=JSON %s\"%(mediafile)\n",
    "    proc = subprocess.Popen(cmd, shell=True,stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, stderr = proc.communicate()\n",
    "    data = json.loads(stdout)\n",
    "    return data\n",
    "\n",
    "\n",
    "def getDuration(mediafile):\n",
    "    '''\n",
    "    getDuration function get the duration of a mediafile, typically a video\n",
    "    Input: a mediafile, on which we then extract the mediainfo\n",
    "    Output: the duration of the media\n",
    "    '''\n",
    "    data = getMediaInfo(mediafile)\n",
    "    duration = float(data['media']['track'][0]['Duration'])\n",
    "    return duration\n",
    "\n",
    "\n",
    "def createTime(time):\n",
    "    '''\n",
    "    createTime function creates a timestamp by adding 1 seconds to input\n",
    "    Input: a time value, as datetime python format\n",
    "    Output: the newly created timestamp\n",
    "    '''\n",
    "    new_time = time\n",
    "    new_time = new_time + timedelta(seconds=1)\n",
    "    return new_time\n",
    "\n",
    "def createLatitude(lat1,lat2):\n",
    "    '''\n",
    "    createLatitude function creates a new Latitude by averaging two others\n",
    "    Input: lat1 and lat2, 2 x latitudes\n",
    "    Output: the average latitude\n",
    "    '''\n",
    "    new_latitude = (lat1+lat2)/2\n",
    "    new_latitude = round(new_latitude,6)\n",
    "    return new_latitude\n",
    "\n",
    "def createLongitude(long1,long2):\n",
    "    '''\n",
    "    createLongitude function creates a new Longitude by averaging two others\n",
    "    Input: long1 and long2, 2 x Longitudes\n",
    "    Output: the average Longitude\n",
    "    '''\n",
    "    new_longitude = (long1+long2)/2\n",
    "    new_longitude = round(new_longitude,6)\n",
    "    return new_longitude\n",
    "\n",
    "def createElevation(elev1,elev2):\n",
    "    '''\n",
    "    createElevation function creates a new Elevation by averaging two others\n",
    "    Input: elev1 and elev2, 2 x Elevations\n",
    "    Output: the average Elevation\n",
    "    '''\n",
    "    new_elevation = (elev1+elev2)/2\n",
    "    new_elevation = round(new_elevation,6)\n",
    "    return new_elevation\n",
    "\n",
    "\n",
    "def fillGPS(inputGPSList,videoLength):\n",
    "    '''\n",
    "    fillGPS function will complete a list of GPS point, by filling in missing points in time series\n",
    "    Input: \n",
    "     - a GPS point list, that comes from the output of the gpsPointList function\n",
    "     - the related video length, from which GPS data is extracted. Value is given by getDuration\n",
    "    Output:\n",
    "    '''\n",
    "    filledGps = inputGPSList.copy()\n",
    "    gps_length = len(filledGps)\n",
    "    iteration_length = int((filledGps[gps_length-1]['Time'] - filledGps[0]['Time']).total_seconds())\n",
    "    ## this section output a filled gps list of length iteration_length+1 = Delta T between last gps timestamp and first one\n",
    "    i = 0\n",
    "    while i < (iteration_length):\n",
    "        delta = filledGps[i+1]['Time']-filledGps[i]['Time']\n",
    "        delta = int(delta.total_seconds())\n",
    "        if delta > 1: # adding a newly created element at index i+1\n",
    "            missing_time = createTime(filledGps[i]['Time'])\n",
    "            missing_latitude = createLatitude(filledGps[i]['Latitude'],filledGps[i+1]['Latitude'])\n",
    "            missing_longitude = createLongitude(filledGps[i]['Longitude'],filledGps[i+1]['Longitude'])\n",
    "            missing_elevation = createElevation(filledGps[i]['Elevation'],filledGps[i+1]['Elevation'])\n",
    "            new_gps = {'Time':missing_time,'Latitude':missing_latitude,'Longitude':missing_longitude,'Elevation':missing_elevation}\n",
    "            filledGps.insert(i+1,new_gps)\n",
    "        i = i+1\n",
    "    ## this section add missing point at the end of the list, in case filledGps initial Delta time length is less than actual video length\n",
    "    if len(filledGps) < videoLength:\n",
    "        j = 0\n",
    "        while len(filledGps) < videoLength:\n",
    "            filledGps.insert(len(filledGps),filledGps[len(filledGps)-1])\n",
    "            j = j+1\n",
    "\n",
    "    return filledGps\n",
    "\n",
    "\n",
    "def longLat2shapePoint(gpsLongLatPoint):\n",
    "    '''\n",
    "    longLat2shapePoint function creats a GPS point with a 'the_geom' key instead of Long/Lat pair\n",
    "    Input: a GPS Point with 'Longitude' and 'Latitude' keys\n",
    "    Output: a dictionnary for a GPS data with key 'the_geom' built from Long/Lat\n",
    "    '''\n",
    "    gpsShapePoint = {'Time':gpsLongLatPoint['Time'],'the_geom':Point(gpsLongLatPoint['Longitude'],gpsLongLatPoint['Latitude']),'Elevation':gpsLongLatPoint['Elevation']}\n",
    "    return gpsShapePoint\n",
    "\n",
    "\n",
    "def longLat2shapeList(gpsLongLatList):\n",
    "    '''\n",
    "    longLat2shapeList function creates a new GPS Point list with 'the_geom' key instead of LongLat\n",
    "    Input: a gpsLongLatList that comes from fillGPS, as we expect the missing fill operation done\n",
    "    Output: a new GPS point list with 'the_geom' key\n",
    "    '''\n",
    "    gpsShapeList = []\n",
    "    for gpsPoint in gpsLongLatList:\n",
    "        gpsShapePoint = longLat2shapePoint(gpsPoint)\n",
    "        gpsShapeList.append(gpsShapePoint)\n",
    "    return gpsShapeList\n",
    "\n",
    "\n",
    "def geometryTransfo(gpsShapePoint):\n",
    "    '''\n",
    "    geometryTransfo function convert a GPS point list from a geo representation to another\n",
    "    Input: a GPS shape point, meaning, a dictionnary with 'the_geom' key instead of LongLat\n",
    "    Output: a GPS shape point with the target geometry, here 2154\n",
    "    '''\n",
    "    project = partial(\n",
    "    pyproj.transform,\n",
    "    pyproj.Proj(init='epsg:4326'), # source coordinate system\n",
    "    pyproj.Proj(init='epsg:2154')) # destination coordinate system\n",
    "\n",
    "    geo1 = gpsShapePoint['the_geom']\n",
    "    geo2 = transform(project,geo1)\n",
    "    return geo2\n",
    "\n",
    "\n",
    "def gps2154(gpsShapePointsFilled):\n",
    "    '''\n",
    "    gps2154 function transforms a GPS shape point list, into the 2154 geometry\n",
    "    Input: a GPS shape point list where GPS point source geometry is 4326\n",
    "    Output: a GPS shape point list where GPS point target geometry is 2154\n",
    "    '''\n",
    "    gps2154Points = []\n",
    "    for point in tqdm(gpsShapePointsFilled):\n",
    "        geo2154 = geometryTransfo(point)\n",
    "        gps2154Point = {'Time':point['Time'],'the_geom':geo2154,'Elevation':point['Elevation']}\n",
    "        gps2154Points.append(gps2154Point)\n",
    "    return gps2154Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostGre Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "\n",
    "def pgConnectionString():\n",
    "    '''\n",
    "    pgConnectionString function creates the connection string to connect to PostGre server\n",
    "    Input: none\n",
    "    Output: the connection string\n",
    "    '''\n",
    "    pgserver = os.getenv(\"PGSERVER\")\n",
    "    pgdatabase = os.getenv(\"PGDATABASE\")\n",
    "    pgusername = os.getenv(\"PGUSERNAME\")\n",
    "    pgpassword = os.getenv(\"PGPWD\")\n",
    "    sslmode = \"require\"\n",
    "    conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(pgserver, pgusername, pgdatabase, pgpassword, sslmode)\n",
    "    return conn_string\n",
    "\n",
    "def pgOpenConnection(conn_string):\n",
    "    '''\n",
    "    pgOpenConnection function open a connection to PostGre server\n",
    "    Input: a connection string formated for PG server, from pgConnectionString output\n",
    "    Output: in case successful, a postgre connection object\n",
    "    '''\n",
    "    try:\n",
    "        conn = psycopg2.connect(conn_string)\n",
    "        print(\"Connection established\")\n",
    "        return conn\n",
    "    except psycopg2.OperationalError as err:\n",
    "        print(\"Connection could not established: \",err)\n",
    "\n",
    "\n",
    "def pgCloseConnection(connection):\n",
    "    '''\n",
    "    pgCloseConnection function closes a connection to a PG server\n",
    "    Input: a PostGre connection object, output from pgOpenConnection\n",
    "    Output: None\n",
    "    '''\n",
    "    try:\n",
    "        connection.close()\n",
    "        print(\"PG connection closed\")\n",
    "    except:\n",
    "        print(\"PG connection could not close successfully\")\n",
    "\n",
    "\n",
    "def trashGPS(trashId,gps2154Points):\n",
    "    '''\n",
    "    trashGPS is a dummy helper function that allows to associate a GPS point to a trashId\n",
    "    This function is expected to be replaced by another one, taking real trash index in video to map correct GPS point.\n",
    "    Input: a trashId from AI prediction dictionnary\n",
    "    Output: a list of GPS Point in 2154 geometry\n",
    "    '''\n",
    "    length = len(gps2154Points)+1\n",
    "    gpsIndex = trashId % length\n",
    "    return gpsIndex\n",
    "\n",
    "\n",
    "def trashInsert(gps2154Point,trashTypeId,cursor,connexion):\n",
    "    '''\n",
    "    trashInsert function is the actual INSERT of a Trash detected by AI within PostGre Trash Table\n",
    "    Input: a gps2154Point, a TrashTypeId, a postgre cursor, a postgre connection\n",
    "    Output: the row_id within Trash Table of the Trash which has just been inserted\n",
    "    '''\n",
    "    point = gps2154Point['the_geom'].wkt\n",
    "    elevation = gps2154Point['Elevation']\n",
    "    timestamp = gps2154Point['Time']\n",
    "    cursor.execute(\"INSERT INTO campaign.trash (id, id_ref_campaign_fk,the_geom, elevation, id_ref_trash_type_fk,brand_type,time ) VALUES (DEFAULT, '1faaee65-1edb-45ab-bdd4-15268fccd301',ST_SetSRID(%s::geometry,2154),%s,%s,%s,%s) RETURNING id;\", (point,elevation,trashTypeId,'icetea',timestamp))\n",
    "    connexion.commit()\n",
    "    row_id = cursor.fetchone()[0]\n",
    "    return row_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: getPrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_name_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-08eb980b798d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This test allows to get a prediction on a small video in /tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvideo_path_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/raph/Documents/SurfriderFoundation/data/campaign/28022020_Boudigau/28022020_Boudigau_4/28022020_Boudigau_4.MP4\"\u001b[0m \u001b[0;31m#'28022020_Boudigau_4_short_480.mov'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprediction_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprediction_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video_name_test' is not defined"
     ]
    }
   ],
   "source": [
    "# This test allows to get a prediction on a small video in /tmp\n",
    "video_path_test = \"/Users/raph/Documents/SurfriderFoundation/data/campaign/28022020_Boudigau/28022020_Boudigau_4/28022020_Boudigau_4.MP4\" #'28022020_Boudigau_4_short_480.mov'\n",
    "prediction_test = getPrediction(video_path_test)\n",
    "prediction_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: goproToGPX() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/raph/Documents/SurfriderFoundation/data/campaign/28022020_Boudigau/28022020_Boudigau_4/28022020_Boudigau_4.MP4.gpx'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Video with GPS Data\n",
    "video_path = '/Users/raph/Documents/SurfriderFoundation/data/campaign/28022020_Boudigau/28022020_Boudigau_4/28022020_Boudigau_4.MP4'\n",
    "gpx_path = goproToGPX(video_path)\n",
    "gpx_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: PostGre trashInsert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/28022020_Boudigau_4.MP4.gpx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-248ba34885e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# GPX parsing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgpx_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpx_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mgpx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpx_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# data from parsed gpx file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# GPS Points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/28022020_Boudigau_4.MP4.gpx'"
     ]
    }
   ],
   "source": [
    "# This test allows to validate Insert within Postgre\n",
    "# This test has getPrediction() and goproToGPX() tests prequesite\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# GPX parsing\n",
    "gpx_file = open(gpx_path,'r',encoding='utf-8')\n",
    "gpx_data = gpxpy.parse(gpx_file) # data from parsed gpx file\n",
    "# GPS Points\n",
    "gpsPoints = gpsPointList(gpx_data)\n",
    "# GPS Point test\n",
    "gps_point_test = gpsPoints[0]\n",
    "print(gps_point_test)\n",
    "# GPS Shape Point test\n",
    "gps_shape_point_test = longLat2shapePoint(gps_point_test)\n",
    "print(gps_shape_point_test)\n",
    "print(gps_shape_point_test['the_geom'].wkt)\n",
    "# GPS 2154 test\n",
    "geo2154 = geometryTransfo(gps_shape_point_test)\n",
    "gps_2154_point_test = {'Time':gps_shape_point_test['Time'],'the_geom':geo2154,'Elevation':gps_shape_point_test['Elevation']}\n",
    "print(gps_2154_point_test)\n",
    "# PG connection_string, connection, cursor\n",
    "pgConn_string = pgConnectionString()\n",
    "pgConnection = pgOpenConnection(pgConn_string)\n",
    "pgCursor = pgConnection.cursor()\n",
    "# trashInsert() Test\n",
    "rowID = trashInsert(gps_2154_point_test,1,pgCursor,pgConnection)\n",
    "print(rowID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def main():\n",
    "    print('############################################################')\n",
    "    print('################ Plastic Origin ETL process ################')\n",
    "    print('################  Let\\'s predict some Trash  ################')\n",
    "    print('############################################################')\n",
    "    print('\\n')\n",
    "\n",
    "    print('###################### Pipeline Step0 ######################')\n",
    "    print('################ Get Video from Azure Storage ##############')\n",
    "    # blob storage connection string\n",
    "    connection_string = os.getenv(\"CONN_STRING\")\n",
    "\n",
    "    # get list of blobs in container campaign0\n",
    "    campaign_container_name = 'campaign0'\n",
    "    blobs_campaign0 = blobInContainer(connection_string,campaign_container_name)\n",
    "    print(\"Blobs in container:\")\n",
    "    print(blobs_campaign0)\n",
    "\n",
    "    # get infos of blob 'goproshort-480p.mov' '28022020_Boudigau_4_short.mp4'\n",
    "    blob_video_name = 'goproshort-480p.mov'   \n",
    "    blobInfos(connection_string,campaign_container_name,blob_video_name)\n",
    "\n",
    "    # download locally in /tmp blob video\n",
    "    blob_video = BlobClient.from_connection_string(conn_str=connection_string,container_name=campaign_container_name, blob_name=blob_video_name)\n",
    "    downloadBlob(blob_video)\n",
    "\n",
    "    print('###################### Pipeline Step1bis ###################')\n",
    "    print('##################### AI Trash prediction ##################')\n",
    "\n",
    "    isAIready = AIready('http://aiapisurfrider.northeurope.cloudapp.azure.com:5000')\n",
    "    logger =  logging.getLogger() #required by getPrediction()\n",
    "\n",
    "    if isAIready == True:\n",
    "        prediction = getPrediction(blob_video_name)\n",
    "    else:\n",
    "        print(\"Early exit of ETL workflow as AI service is not available\")\n",
    "        exit()\n",
    "\n",
    "    '''AIready('http://aiapisurfrider.northeurope.cloudapp.azure.com:5000')\n",
    "    # get predictions from AI on goproshort-480p.mov\n",
    "    prediction = getPrediction(blob_video_name)'''\n",
    "\n",
    "    # cast prediction to JSON/Dictionnary format\n",
    "    json_prediction = jsonPrediction(prediction)\n",
    "\n",
    "    print('###################### Pipeline Step1 ######################')\n",
    "    print('######################  GPX creation  ######################')\n",
    "    video_name = '28022020_Boudigau_4.MP4'\n",
    "    gpx_path = goproToGPX(video_name)\n",
    "\n",
    "    # GPX parsing\n",
    "    gpx_file = open(gpx_path,'r',encoding='utf-8')\n",
    "    gpx_data = gpxpy.parse(gpx_file) # data from parsed gpx file\n",
    "\n",
    "    # GPS Points\n",
    "    gpsPoints = gpsPointList(gpx_data)\n",
    "\n",
    "    # Video duration\n",
    "    print(\"\\n\")\n",
    "    video_duration = getDuration('/tmp/'+video_name)\n",
    "    print(\"Video duration in second from metadata:\",video_duration)\n",
    "\n",
    "    # GPS file duration\n",
    "    timestampDelta = gpsPoints[len(gpsPoints)-1]['Time'] - gpsPoints[0]['Time']\n",
    "    print(\"GPS file time coverage in second: \",timestampDelta.seconds)\n",
    "\n",
    "    print('###################### Pipeline Step2 ######################')\n",
    "    print('################## Add missing GPS points ##################')\n",
    "    video_duration_sup = int(video_duration)+1\n",
    "    gpsPointsFilled = fillGPS(gpsPoints,video_duration_sup)\n",
    "\n",
    "    print('###################### Pipeline Step3 ######################')\n",
    "    print('############ Transformation to GPS Shape Points ############')\n",
    "    gpsShapePointsFilled = longLat2shapeList(gpsPointsFilled)\n",
    "\n",
    "    print('###################### Pipeline Step4 ######################')\n",
    "    print('############## Transformation to 2154 Geometry #############')\n",
    "    gps2154PointsFilled = gps2154(gpsShapePointsFilled)\n",
    "\n",
    "    print('###################### Pipeline Step5 ######################')\n",
    "    print('################### Insert within PostGre ##################')\n",
    "    \n",
    "    # Get connection string information from env variables\n",
    "    pgConn_string = pgConnectionString()\n",
    "    # Open pgConnection\n",
    "    pgConnection = pgOpenConnection(pgConn_string)\n",
    "    # Create Cursor\n",
    "    pgCursor = pgConnection.cursor()\n",
    "\n",
    "\n",
    "    # INSERTING all detected_trash within PostGre\n",
    "    rowID_list = []\n",
    "    for prediction in tqdm(json_prediction['detected_trash']):\n",
    "        try: \n",
    "            # get GPS coordinate\n",
    "            trashTypeId= prediction['id']\n",
    "            gpsIndexId = trashGPS(trashTypeId,gps2154PointsFilled)\n",
    "            trashGps2154Point = gps2154PointsFilled[gpsIndexId]\n",
    "            # get TrashTypeId from AI prediction\n",
    "            label = getTrashLabel(prediction)\n",
    "            trashType = mapLabel2TrashIdPG(label)\n",
    "            # INSERT within PostGRE\n",
    "            rowID = trashInsert(trashGps2154Point,trashType,pgCursor,pgConnection)\n",
    "            rowID_list.append(rowID)\n",
    "        except:\n",
    "            print(\"There was an issue inserting Trash id:\" + str(prediction['id']) + \" within PostGre\")\n",
    "    print(\"Successfully inserted \" + str(len(rowID_list)) + \" Trashes within Trash table\")    \n",
    "\n",
    "    # Close PG connection\n",
    "    pgCloseConnection(pgConnection)\n",
    "\n",
    "    print('############################################################')\n",
    "    print('################   Plastic Origin ETL End   ################')\n",
    "    print('############################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "################ Plastic Origin ETL process ################\n",
      "################  Let's predict some Trash  ################\n",
      "############################################################\n",
      "\n",
      "\n",
      "###################### Pipeline Step0 ######################\n",
      "################ Get Video from Azure Storage ##############\n",
      "The container you are trying to list blob from probably does not exist.\n",
      "Early exit of ETL process as container probably does not exist.\n",
      "Blobs in container:\n",
      "None\n",
      "The blob you are trying to get info from probably does not exist\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'rstrip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-892c151d6ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Execute main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-a5c256952a9a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# download locally in /tmp blob video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mblob_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlobClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_connection_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontainer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcampaign_container_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblob_video_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mdownloadBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/etl-env/lib/python3.6/site-packages/azure/storage/blob/_blob_client.py\u001b[0m in \u001b[0;36mfrom_connection_string\u001b[0;34m(cls, conn_str, container_name, blob_name, snapshot, credential, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;34m:\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCreating\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mBlobClient\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0maccount_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_connection_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'secondary_hostname'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'secondary_hostname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecondary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/etl-env/lib/python3.6/site-packages/azure/storage/blob/_shared/base_client.py\u001b[0m in \u001b[0;36mparse_connection_str\u001b[0;34m(conn_str, credential, service)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_connection_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mconn_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     \u001b[0mconn_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconn_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconn_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'rstrip'"
     ]
    }
   ],
   "source": [
    "# Execute main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main with Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    ######## Pipeline Step0: Get Video to predict and insert#########\n",
    "    print('######## Pipeline Step0: Get Video from Azure Blob Storage #########')\n",
    "    # blob storage connection string\n",
    "    connection_string = os.getenv(\"CONN_STRING\")\n",
    "\n",
    "    # get list of blobs in container campaign0\n",
    "    campaign_container_name = argv.containername\n",
    "    blobs_campaign0 = blobInContainer(connection_string,campaign_container_name)\n",
    "\n",
    "    # get infos of blob 'goproshort-480p.mov' '28022020_Boudigau_4_short.mp4'\n",
    "    blob_video_name = argv.blobname   \n",
    "    blobInfos(connection_string,campaign_container_name,blob_video_name)\n",
    "\n",
    "    # download locally in /tmp blob video\n",
    "    blob_video = BlobClient.from_connection_string(conn_str=connection_string,container_name=campaign_container_name, blob_name=blob_video_name)\n",
    "    downloadBlob(blob_video)\n",
    "\n",
    "    ######## Pipeline Step 1bis: AI Trash prediction #########\n",
    "    print('######## Pipeline Step 1bis: AI Trash prediction #########')\n",
    "\n",
    "    isAIready = AIready('http://aiapisurfrider.northeurope.cloudapp.azure.com:5000')\n",
    "\n",
    "    if isAIready == True:\n",
    "        prediction = getPrediction(blob_video_name)\n",
    "    else:\n",
    "        print(\"Early exit of ETL workflow as AI service is not available\")\n",
    "        exit()\n",
    "\n",
    "    # cast prediction to JSON/Dictionnary format\n",
    "    json_prediction = jsonPrediction(prediction)\n",
    "\n",
    "    ######## Pipeline Step 1: GPX creation ########\n",
    "    print('######## Pipeline Step 1: GPX creation ########')\n",
    "    # IMPORTANT REMARK: video_name should be equal to blob name in the future\n",
    "    # For now, because of video size and internet bandwidth, blob name is a shorten low quality form of the video\n",
    "    # As a consequence, it does not include GPS information that could be process then\n",
    "    video_name = argv.videoname\n",
    "    gpx_path = goproToGPX(video_name)\n",
    "\n",
    "    # GPX parsing\n",
    "    gpx_file = open(gpx_path,'r',encoding='utf-8')\n",
    "    gpx_data = gpxpy.parse(gpx_file) # data from parsed gpx file\n",
    "\n",
    "    # GPS Points\n",
    "    gpsPoints = gpsPointList(gpx_data)\n",
    "\n",
    "    # Video duration\n",
    "    print(\"\\n\")\n",
    "    video_duration = getDuration('/tmp/'+video_name)\n",
    "    print(\"Video duration in second from metadata:\",video_duration)\n",
    "\n",
    "    # GPS file duration\n",
    "    timestampDelta = gpsPoints[len(gpsPoints)-1]['Time'] - gpsPoints[0]['Time']\n",
    "    print(\"GPS file time coverage in second: \",timestampDelta.seconds)\n",
    "\n",
    "    ######## Pipeline Step 2: Create gpsPointFilled ########\n",
    "    print('######## Pipeline Step 2: Add missing GPS points ########')\n",
    "    video_duration_sup = int(video_duration)+1\n",
    "    gpsPointsFilled = fillGPS(gpsPoints,video_duration_sup)\n",
    "\n",
    "    ######## Pipeline Step 3: Transform to GPS shapePoints ########\n",
    "    print('######## Pipeline Step 3: Transformation to GPS Shape Points ########')\n",
    "    gpsShapePointsFilled = longLat2shapeList(gpsPointsFilled)\n",
    "\n",
    "    ######## Pipeline Step 4: Transform to 2154 Geometry ########\n",
    "    print('######## Pipeline Step 4: Transformation to 2154 Geometry ########')\n",
    "    gps2154PointsFilled = gps2154(gpsShapePointsFilled)\n",
    "\n",
    "    ######## Pipeline Step 5: Insert within PostGre ########\n",
    "    print('######## Pipeline Step 5: Insert within PostGre ########')\n",
    "    \n",
    "    # Get connection string information from env variables\n",
    "    pgConn_string = pgConnectionString()\n",
    "    # Open pgConnection\n",
    "    pgConnection = pgOpenConnection(pgConn_string)\n",
    "    # Create Cursor\n",
    "    pgCursor = pgConnection.cursor()\n",
    "\n",
    "\n",
    "    # INSERTING all detected_trash within PostGre\n",
    "    rowID_list = []\n",
    "    for prediction in tqdm(json_prediction['detected_trash']):\n",
    "        try: \n",
    "            # get GPS coordinate\n",
    "            trashTypeId= prediction['id']\n",
    "            gpsIndexId = trashGPS(trashTypeId,gps2154PointsFilled)\n",
    "            trashGps2154Point = gps2154PointsFilled[gpsIndexId]\n",
    "            # get TrashTypeId from AI prediction\n",
    "            label = getTrashLabel(prediction)\n",
    "            trashType = mapLabel2TrashIdPG(label)\n",
    "            # INSERT within PostGRE\n",
    "            rowID = trashInsert(trashGps2154Point,trashType,pgCursor,pgConnection)\n",
    "            rowID_list.append(rowID)\n",
    "        except:\n",
    "            print(\"There was an issue inserting Trash id:\" + str(prediction['id']) + \" within PostGre\")\n",
    "    print(\"Successfully inserted \" + str(len(rowID_list)) + \" Trashes within Trash table\")    \n",
    "\n",
    "    # Close PG connection\n",
    "    pgCloseConnection(pgConnection)\n",
    "\n",
    "        print('############################################################')\n",
    "    print('################   Plastic Origin ETL End   ################')\n",
    "    print('############################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--containername', help='container name to get blob info from and download blob from to be processed by ETL')\n",
    "parser.add_argument('--blobname', help='blob name to be downloaded from azure blob storage campaign0 container into /tmp')\n",
    "parser.add_argument('--videoname', help='video name stored locally in /tmp to apply gpx extraction process on')\n",
    "\n",
    "\n",
    "# Create args parsing standard input\n",
    "try:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Launch ETL execution\n",
    "    if (args.containername == None or args.blobname == None or args.videoname == None):\n",
    "        print(\"Please provide containername, blobname and videoname arguments as they are all mandatory to execute ETL process.\")\n",
    "    else:\n",
    "        # Execute main function only if \n",
    "        if __name__ == '__main__':\n",
    "            main(args)\n",
    "\n",
    "except SystemExit:\n",
    "    print(\"There was an issue parsing arguments\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
