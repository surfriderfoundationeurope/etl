{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitsurfridercondaa4d29431e9f947419d6d850056118d13",
   "display_name": "Python 3.6.9 64-bit ('surfrider': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trash End to End - SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "from azure.storage.blob import BlobClient\n",
    "import json \n",
    "import os\n",
    "import pyodbc\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' blobContainer create a name_list of blobs within container'''\n",
    "''' params are container name (no full url) & storage conn string'''\n",
    "def blobInContainer(connection_s,container_n):\n",
    "    campaign_container = ContainerClient.from_connection_string(conn_str=connection_s, container_name=container_n)\n",
    "    blob_list = campaign_container.list_blobs()\n",
    "    blob_names_list = []\n",
    "    for blob in blob_list:\n",
    "        blob_names_list.append(blob.name)\n",
    "    return blob_names_list\n",
    "\n",
    "''' blobInfos provides basic information about a blob object'''\n",
    "''' params are blob_name only (no full url) & storage conn string'''\n",
    "def blobInfos(connection_s,container_n,blob_n):\n",
    "    blob_video = BlobClient.from_connection_string(conn_str=connection_s,container_name=container_n, blob_name=blob_n)\n",
    "    blob_video_url = blob_video.url\n",
    "    blob_video_prop = blob_video.get_blob_properties()\n",
    "    blob_video_prop_keys = blob_video_prop.keys()\n",
    "    print(\"blob name:\",blob_n)\n",
    "    print(\"blob URL:\",blob_video_url)\n",
    "    print(\"blob properties:\", blob_video_prop)\n",
    "    print(\"blob properties keys:\", blob_video_prop_keys)\n",
    "\n",
    "\n",
    "''' downloadBlob from Azure to local file system'''\n",
    "''' parameter is a blob client object from azure storage sdk'''\n",
    "def downloadBlob(blobclient):\n",
    "    with open(\"/tmp/\"+blobclient.blob_name, \"wb\") as my_blob_dl:\n",
    "        blob_data = blobclient.download_blob()\n",
    "        blob_data.readinto(my_blob_dl)\n",
    "    print(\"Blob %s downloaded\" %blobclient.blob_name)\n",
    "    print(\"Blob path: /tmp/%s\" %blobclient.blob_name)\n",
    "\n",
    "\n",
    "''' getPrediction function sends curl request to Surfrider AI'''\n",
    "''' video name is the name of a locally downloaded video from Azure'''\n",
    "''' video name is passed as an argument to curl_request script '''\n",
    "''' curl_request script sends actual POST request to Surfrider AI'''\n",
    "def getPrediction(video_name):\n",
    "    curl_request_script = ['./curl_request_param.sh',video_name]\n",
    "    output = []\n",
    "    request_answer = subprocess.Popen(curl_request_script, stdout=subprocess.PIPE)\n",
    "    i = 0\n",
    "    for line in request_answer.stdout:\n",
    "        print(line)\n",
    "        output.append(line)\n",
    "    return output\n",
    "\n",
    "\n",
    "''' jsonPrediction cast a prediction from getPrediction function '''\n",
    "''' from a list prediction to a string then dictionnary with json_loads '''\n",
    "def jsonPrediction(pred):\n",
    "    string_prediction = str(pred[0])[2:-3] #removing 2 x first and 3 last characters of pred\n",
    "    json_prediction = json.loads(string_prediction)\n",
    "    return json_prediction\n",
    "\n",
    "\n",
    "''' getTrashLabel return label from a frame_to_box'''\n",
    "def getTrashLabel(frame_2_box):\n",
    "    return frame_2_box['label']\n",
    "\n",
    "''' mapLabelTrashId is a switch that converts label to TrashId'''\n",
    "''' param is label that comes from AI predictions dictionnary jsonPrediction'''\n",
    "def mapLabel2TrashId(label):\n",
    "    switcher = { \n",
    "    \"Fishing or Hunting\":\"89B44BAA-69AA-4109-891A-128E012E7E07\",\n",
    "    \"Food Packaging\":\"185FEFA2-EEF2-47A8-873E-26032A4BB3C3\",\n",
    "    \"Unknown\":\"BB4DEA69-218A-40CC-A000-2AE17C37152C\",\n",
    "    \"Industrial or Construction Debris\":\"2A863E38-E5D0-455F-87CE-2B75DA29F59A\",\n",
    "    \"fragments\":\"ED401B92-DC24-44C0-A52A-34CE831092BF\",\n",
    "    \"Agricultural Waste\":\"36B2AFEB-7A7C-44B5-A790-5E5C73BA144D\",\n",
    "    \"others\":\"4BEC18FC-BC48-45B7-AFDA-6BA96BD80921\",\n",
    "    \"Common Household Items\":\"C68E90CF-6E65-4474-BC60-72E1C8513F55\",\n",
    "    \"plastic\":\"6961D0DB-928C-419E-9985-98EEEAF552C7\",\n",
    "    \"bottles\":\"9780940B-D06C-4AAB-8003-AB914981E87A\",\n",
    "    \"Drinking Bottles\":\"BCF549A8-AECD-4BC9-B9B8-B94A8F3758D5\",\n",
    "    \"Unknown10\":\"BC7BB564-BE04-4B4B-9913-FF69780B93A6\"\n",
    "    } \n",
    "    return switcher.get(label, \"nothing\")\n",
    "\n",
    "\n",
    "''' trashInsert execute INSERT request to dbo.Trash table'''\n",
    "''' parameter to indsert is a trashTypeId defined in Trash_Type table'''\n",
    "def trashInsert(cursor,cnxn,trashTypeId):\n",
    "    cursor.execute(\"INSERT dbo.Trash (Id,CampaignId,Latitude,Longitude,TrashTypeId,Precision,AI_Version) VALUES (NEWID(),'8D21A132-CF4B-404E-B287-C40A2F12D305','50.797731', '2.179029', ? ,'0.95','0')\",trashTypeId)\n",
    "    cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # blob storage connection string\n",
    "    connection_string = os.getenv(\"CONN_STRING\")\n",
    " \n",
    "    # get list of blobs in container campaign0\n",
    "    campaign_container_name = 'campaign0'\n",
    "    #campaign_container_name = 'campaign1'\n",
    "    blobs_campaign0 = blobInContainer(connection_string,campaign_container_name)\n",
    "    print(blobs_campaign0)\n",
    "\n",
    "    # get infos of blob video vid1-4K.MP4,vid1-4K2.MP4,vid1-4K3.mp4,vid1-4K4.mp4,19022020_Adour_6.mp4\n",
    "#    blob_video_name = '19022020_Adour_6.mp4'\n",
    "    blob_video_name = 'vid1-480p.mov'   \n",
    "    blobInfos(connection_string,campaign_container_name,blob_video_name)\n",
    "\n",
    "    # download blob video\n",
    "    blob_video0 = BlobClient.from_connection_string(conn_str=connection_string,container_name=campaign_container_name, blob_name=blob_video_name)\n",
    "    downloadBlob(blob_video0)\n",
    "\n",
    "    # get predictions from AI on vid1-4K4.MP4\n",
    "    prediction = getPrediction(blob_video_name)\n",
    "\n",
    "    # cast prediction to JSON/Dictionnary format\n",
    "    json_prediction = jsonPrediction(prediction)\n",
    "    json_prediction\n",
    "\n",
    "    # SQL connection\n",
    "    server = os.getenv(\"SERVER\")\n",
    "    database = os.getenv(\"DATABASE\")\n",
    "    username = os.getenv(\"USERNAME\")\n",
    "    password = os.getenv(\"SQLPWD\")\n",
    "    driver= '{ODBC Driver 17 for SQL Server}'\n",
    "    cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    # Executing INSERT for all trash detected in json_prediction from vid1-4K4.MP4\n",
    "    for frame_to_box in json_prediction['detected_trash']:\n",
    "        print(frame_to_box)\n",
    "        trashLabel = getTrashLabel(frame_to_box)\n",
    "        print(trashLabel)\n",
    "        mapLabel = mapLabel2TrashId(trashLabel)\n",
    "        print(mapLabel)\n",
    "        trashInsert(cursor,cnxn,mapLabel)\n",
    "    \n",
    "    # Visualize main result within dbo.Trash Table\n",
    "    cursor.execute(\"SELECT * FROM dbo.Trash\")\n",
    "    row = cursor.fetchone()\n",
    "    while row:\n",
    "        print (str(row[0]))\n",
    "        row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostGre SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pgserver = os.getenv(\"PGSERVER\")\n",
    "pgdatabase = os.getenv(\"PGDATABASE\")\n",
    "pgusername = os.getenv(\"PGUSERNAME\")\n",
    "pgpassword = os.getenv(\"PGPWD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "\n",
    "# Update connection string information from env variables\n",
    "pgserver = os.getenv(\"PGSERVER\")\n",
    "pgdatabase = os.getenv(\"PGDATABASE\")\n",
    "pgusername = os.getenv(\"PGUSERNAME\")\n",
    "pgpassword = os.getenv(\"PGPWD\")\n",
    "sslmode = \"require\"\n",
    "\n",
    "# Construct connection string with psycopg2\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(pgserver, pgusername, pgdatabase, pgpassword, sslmode)\n",
    "conn = psycopg2.connect(conn_string) \n",
    "print(\"Connection established\")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first x 10 records of PostGre public.Trash table\n",
    "cursor.execute(\"SELECT * FROM public.Trash\")\n",
    "row = cursor.fetchone()\n",
    "\n",
    "for k in range(0,10):\n",
    "    for i in range(0,5):\n",
    "        print (str(row[i]))\n",
    "    row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT REQUEST: point parameter\n",
    "def trashInsert(the_geom,cursor,connexion):\n",
    "    point = 'POINT(' + str(the_geom.coords.xy[0][0]) + ' ' + str(the_geom.coords.xy[1][0]) + ')'\n",
    "    cursor.execute(\"INSERT INTO public.trash (id, id_ref_campaign_fk,the_geom, elevation, id_ref_trash_type_fk,brand_type ) VALUES (DEFAULT, 'fb367e0e-9b56-4418-9b80-5acd5c239754',%s,'610.109375',1,%s) RETURNING id;\", (point,'oasis'))\n",
    "    connexion.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Trash Insert with shapely python Point in PostGre\n",
    "from shapely.geometry import Point\n",
    "geopoint = Point(0.0,1.0)\n",
    "trashInsert(geopoint,cursor,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trash End to End - PostGre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "from azure.storage.blob import BlobClient\n",
    "import json \n",
    "import os\n",
    "import pyodbc\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract & Transform: IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' blobContainer create a name_list of blobs within container'''\n",
    "''' params are container name (no full url) & storage conn string'''\n",
    "def blobInContainer(connection_s,container_n):\n",
    "    campaign_container = ContainerClient.from_connection_string(conn_str=connection_s, container_name=container_n)\n",
    "    blob_list = campaign_container.list_blobs()\n",
    "    blob_names_list = []\n",
    "    for blob in blob_list:\n",
    "        blob_names_list.append(blob.name)\n",
    "    return blob_names_list\n",
    "\n",
    "''' blobInfos provides basic information about a blob object'''\n",
    "''' params are blob_name only (no full url) & storage conn string'''\n",
    "def blobInfos(connection_s,container_n,blob_n):\n",
    "    blob_video = BlobClient.from_connection_string(conn_str=connection_s,container_name=container_n, blob_name=blob_n)\n",
    "    blob_video_url = blob_video.url\n",
    "    blob_video_prop = blob_video.get_blob_properties()\n",
    "    blob_video_prop_keys = blob_video_prop.keys()\n",
    "    print(\"blob name:\",blob_n)\n",
    "    print(\"blob URL:\",blob_video_url)\n",
    "    print(\"blob properties:\", blob_video_prop)\n",
    "    print(\"blob properties keys:\", blob_video_prop_keys)\n",
    "\n",
    "\n",
    "''' downloadBlob from Azure to local file system'''\n",
    "''' parameter is a blob client object from azure storage sdk'''\n",
    "def downloadBlob(blobclient):\n",
    "    with open(\"/tmp/\"+blobclient.blob_name, \"wb\") as my_blob_dl:\n",
    "        blob_data = blobclient.download_blob()\n",
    "        blob_data.readinto(my_blob_dl)\n",
    "    print(\"Blob %s downloaded\" %blobclient.blob_name)\n",
    "    print(\"Blob path: /tmp/%s\" %blobclient.blob_name)\n",
    "    path = \"/tmp/\"+blobclient.blob_name\n",
    "    return path\n",
    "\n",
    "\n",
    "''' getPrediction function sends curl request to Surfrider AI'''\n",
    "''' video name is the name of a locally downloaded video from Azure'''\n",
    "''' video name is passed as an argument to curl_request script '''\n",
    "''' curl_request script sends actual POST request to Surfrider AI'''\n",
    "def getPrediction(video_name):\n",
    "    curl_request_script = ['./curl_request_param.sh',video_name]\n",
    "    output = []\n",
    "    request_answer = subprocess.Popen(curl_request_script, stdout=subprocess.PIPE)\n",
    "    i = 0\n",
    "    for line in request_answer.stdout:\n",
    "        print(line)\n",
    "        output.append(line)\n",
    "    return output\n",
    "\n",
    "\n",
    "''' jsonPrediction cast a prediction from getPrediction function '''\n",
    "''' from a list prediction to a string then dictionnary with json_loads '''\n",
    "def jsonPrediction(pred):\n",
    "    string_prediction = str(pred[0])[2:-3] #removing 2 x first and 3 last characters of pred\n",
    "    json_prediction = json.loads(string_prediction)\n",
    "    return json_prediction\n",
    "\n",
    "\n",
    "''' getTrashLabel return label from a frame_to_box'''\n",
    "def getTrashLabel(frame_2_box):\n",
    "    return frame_2_box['label']\n",
    "\n",
    "''' mapLabelTrashId is a switch that converts label to TrashId'''\n",
    "''' param is label that comes from AI predictions dictionnary jsonPrediction'''\n",
    "def mapLabel2TrashId(label):\n",
    "    switcher = { \n",
    "    \"Fishing or Hunting\":\"89B44BAA-69AA-4109-891A-128E012E7E07\",\n",
    "    \"Food Packaging\":\"185FEFA2-EEF2-47A8-873E-26032A4BB3C3\",\n",
    "    \"Unknown\":\"BB4DEA69-218A-40CC-A000-2AE17C37152C\",\n",
    "    \"Industrial or Construction Debris\":\"2A863E38-E5D0-455F-87CE-2B75DA29F59A\",\n",
    "    \"fragments\":\"ED401B92-DC24-44C0-A52A-34CE831092BF\",\n",
    "    \"Agricultural Waste\":\"36B2AFEB-7A7C-44B5-A790-5E5C73BA144D\",\n",
    "    \"others\":\"4BEC18FC-BC48-45B7-AFDA-6BA96BD80921\",\n",
    "    \"Common Household Items\":\"C68E90CF-6E65-4474-BC60-72E1C8513F55\",\n",
    "    \"plastic\":\"6961D0DB-928C-419E-9985-98EEEAF552C7\",\n",
    "    \"bottles\":\"9780940B-D06C-4AAB-8003-AB914981E87A\",\n",
    "    \"Drinking Bottles\":\"BCF549A8-AECD-4BC9-B9B8-B94A8F3758D5\",\n",
    "    \"Unknown10\":\"BC7BB564-BE04-4B4B-9913-FF69780B93A6\"\n",
    "    } \n",
    "    return switcher.get(label, \"nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def isAIready(url):\n",
    "    AI_ready = requests.get(url)\n",
    "    if AI_ready.status_code == 200:\n",
    "        print('AI is ready')\n",
    "        print('HTTP Status code: ',AI_ready.status_code)\n",
    "        print(AI_ready.headers)\n",
    "    else:\n",
    "        print(\"AI not found, an error has occured\")\n",
    "        print(\"Server Answer: \",AI_ready)\n",
    "        print(\"HTTP Status Code: \",AI_ready.status_code)\n",
    "        print(\"HTTP Status Code: \",AI_ready.raise_for_status())\n",
    "\n",
    "# Video POST request example\n",
    "surfriderVideo = '/tmp/'\n",
    "video = './adour4_short.mp4'\n",
    "video_path = os.path.join(surfriderVideo,video)\n",
    "url = 'http://aiapisurfrider.northeurope.cloudapp.azure.com:5000'\n",
    "files = {'file':(video,open(video ,'rb')),}\n",
    "ai_request = requests.post(url, files = files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_request.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(video ,'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://aiapisurfrider.northeurope.cloudapp.azure.com:5000'\n",
    "isAIready(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob storage connection string\n",
    "connection_string = os.getenv(\"CONN_STRING\")\n",
    "\n",
    "# get list of blobs in container campaign0\n",
    "campaign_container_name = 'campaign0'\n",
    "#campaign_container_name = 'campaign1'\n",
    "blobs_campaign0 = blobInContainer(connection_string,campaign_container_name)\n",
    "print(blobs_campaign0)\n",
    "\n",
    "# get infos of blob video vid1-4K.MP4,vid1-4K2.MP4,vid1-4K3.mp4,vid1-4K4.mp4,19022020_Adour_6.mp4\n",
    "#    blob_video_name = '19022020_Adour_6.mp4'\n",
    "blob_video_name = 'goproshort-480p.mov'   \n",
    "blobInfos(connection_string,campaign_container_name,blob_video_name)\n",
    "\n",
    "# download blob video\n",
    "blob_video0 = BlobClient.from_connection_string(conn_str=connection_string,container_name=campaign_container_name, blob_name=blob_video_name)\n",
    "downloadBlob(blob_video0)\n",
    "\n",
    "# get predictions from AI on vid1-4K4.MP4\n",
    "prediction = getPrediction(blob_video_name)\n",
    "\n",
    "# cast prediction to JSON/Dictionnary format\n",
    "json_prediction = jsonPrediction(prediction)\n",
    "json_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prediction['detected_trash'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract & Transform: GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before getting GPX, make sure GPS Data is included within GoPro File with ffprobe check on\n",
    "# Check whether mp4 to move still contains GPS data ..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create GPX from GoPro video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPX from Video\n",
    "def goproToGPX(video_name):\n",
    "    gopro2gpx_script = ['./gopro2gpx_param.sh',video_name]\n",
    "    result = subprocess.Popen(gopro2gpx_script, stdout=subprocess.PIPE)\n",
    "    output = []\n",
    "    i = 0\n",
    "    for line in result.stdout:\n",
    "        print(line)\n",
    "        output.append(line)\n",
    "    path='/tmp/'+video_name+'.gpx'\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = '28022020_Boudigau_4.MP4'\n",
    "gpx_path = goproToGPX(video_name)\n",
    "gpx_path"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create GPS points from GPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse GPX file\n",
    "import gpxpy\n",
    "import gpxpy.gpx\n",
    "\n",
    "gpx_file = open(gpx_path,'r',encoding='utf-8')\n",
    "gpx_data = gpxpy.parse(gpx_file) # data from parsed gpx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPX points list\n",
    "''' gpsPointList function extract gps points from gpx file '''\n",
    "''' gpxdata is a gpxpy object that returns data from a parsed gpx file'''\n",
    "''' gpsPointList return a list of dictionnary points with Time, Lat, Long, Elev'''\n",
    "def gpsPointList(gpxdata):\n",
    "    point_list = []\n",
    "    for track in gpxdata.tracks:\n",
    "        for segment in track.segments: \n",
    "            for point in segment.points:\n",
    "                point_info = {'Time':point.time,'Latitude':point.latitude,'Longitude':point.longitude,'Elevation':point.elevation}\n",
    "                point_list.append(point_info)\n",
    "    return point_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpsPoints = gpsPointList(gpx_data)\n",
    "print(len(gpsPoints))\n",
    "for gps in gpsPoints:\n",
    "    print(gps)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Video Metadata info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "#===============================\n",
    "def getMediaInfo(mediafile):\n",
    "    cmd = \"mediainfo --Output=JSON %s\"%(mediafile)\n",
    "    proc = subprocess.Popen(cmd, shell=True,stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, stderr = proc.communicate()\n",
    "    data = json.loads(stdout)\n",
    "    return data\n",
    "\n",
    "#===============================\n",
    "def getDuration(mediafile):\n",
    "    data = getMediaInfo(mediafile)\n",
    "    duration = float(data['media']['track'][0]['Duration'])\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video duration\n",
    "video_duration = getDuration('/tmp/'+video_name)\n",
    "print(\"Video duration in second from metadata:\",video_duration)\n",
    "\n",
    "# GPS duration\n",
    "timestampDelta = gpsPoints[len(gpsPoints)-1]['Time'] - gpsPoints[0]['Time']\n",
    "print(\"GPS file time coverage in second: \",timestampDelta.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill in missing GPS timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "def createTime(time):\n",
    "    new_time = time\n",
    "    new_time = new_time + timedelta(seconds=1)\n",
    "    return new_time\n",
    "\n",
    "def createLatitude(lat1,lat2):\n",
    "    new_latitude = (lat1+lat2)/2\n",
    "    new_latitude = round(new_latitude,6)\n",
    "    return new_latitude\n",
    "\n",
    "def createLongitude(long1,long2):\n",
    "    new_longitude = (long1+long2)/2\n",
    "    new_latitude = round(new_longitude,6)\n",
    "    return new_longitude\n",
    "\n",
    "def createElevation(elev1,elev2):\n",
    "    new_elevation = (elev1+elev2)/2\n",
    "    new_elevation = round(new_elevation,6)\n",
    "    return new_elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillGPS(inputGPSList,videoLength):\n",
    "    filledGps = inputGPSList.copy()\n",
    "    gps_length = len(filledGps)\n",
    "    iteration_length = int((filledGps[gps_length-1]['Time'] - filledGps[0]['Time']).total_seconds())\n",
    "    #print(iteration_length)\n",
    "    ## this section output a filled gps list of length iteration_length+1 = Delta T between last gps timestamp and first one\n",
    "    i = 0\n",
    "    while i < (iteration_length):\n",
    "#        print(i)\n",
    "        delta = filledGps[i+1]['Time']-filledGps[i]['Time']\n",
    "        delta = int(delta.total_seconds())\n",
    "        if delta > 1: # adding a newly created element at index i+1\n",
    "            #print(i)\n",
    "            missing_time = createTime(filledGps[i]['Time'])\n",
    "            missing_latitude = createLatitude(filledGps[i]['Latitude'],filledGps[i+1]['Latitude'])\n",
    "            missing_longitude = createLongitude(filledGps[i]['Longitude'],filledGps[i+1]['Longitude'])\n",
    "            missing_elevation = createElevation(filledGps[i]['Elevation'],filledGps[i+1]['Elevation'])\n",
    "            new_gps = {'Time':missing_time,'Latitude':missing_latitude,'Longitude':missing_longitude,'Elevation':missing_elevation}\n",
    "            filledGps.insert(i+1,new_gps)\n",
    "        i = i+1\n",
    "    ## this section add missing point at the end of the list, in case filledGps initial Delta time length is less than actual video length\n",
    "    if len(filledGps) < videoLength:\n",
    "        j = 0\n",
    "        while len(filledGps) < videoLength:\n",
    "            filledGps.insert(len(filledGps),filledGps[len(filledGps)-1])\n",
    "            j = j+1\n",
    "\n",
    "    return filledGps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gpsPointFilled\n",
    "video_duration_sup = int(video_duration)+1\n",
    "gpsPointsFilled = fillGPS(gpsPoints,video_duration_sup)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform LongLat to Point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def longLat2shapePoint(gpsLongLatPoint):\n",
    "    gpsShapePoint = {'Time':gpsLongLatPoint['Time'],'the_geom':Point(gpsLongLatPoint['Longitude'],gpsLongLatPoint['Latitude']),'Elevation':gpsLongLatPoint['Elevation']}\n",
    "    return gpsShapePoint\n",
    "\n",
    "def longLat2shapeList(gpsLongLagList):\n",
    "    gpsShapeList = []\n",
    "    for gpsPoint in gpsLongLagList:\n",
    "        gpsShapePoint = longLat2shapePoint(gpsPoint)\n",
    "        gpsShapeList.append(gpsShapePoint)\n",
    "    return gpsShapeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gpsPoints to gpsShapePoints\n",
    "gpsShapePointsFilled = longLat2shapeList(gpsPointsFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing point in gpsPointsFilled and in gpsShapePointsFilled\n",
    "for i in range(0,20):\n",
    "    print(gpsPoints[i])\n",
    "    print(gpsShapePointsFilled[i]['the_geom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from tqdm import tqdm\n",
    "\n",
    "def geometryTransfo(gpsShapePoint):\n",
    "    project = partial(\n",
    "    pyproj.transform,\n",
    "    pyproj.Proj(init='epsg:4326'), # source coordinate system\n",
    "    pyproj.Proj(init='epsg:2154')) # destination coordinate system\n",
    "\n",
    "    geo1 = gpsShapePoint['the_geom']\n",
    "    geo2 = transform(project,geo1)\n",
    "\n",
    "    return geo2\n",
    "\n",
    "def gps2154(gpsShapePointsFilled):\n",
    "    gps2154Points = []\n",
    "    for point in tqdm(gpsShapePointsFilled):\n",
    "        geo2154 = geometryTransfo(point)\n",
    "        gps2154Point = {'Time':point['Time'],'the_geom':geo2154,'Elevation':point['Elevation']}\n",
    "        gps2154Points.append(gps2154Point)\n",
    "    return gps2154Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing geometryTransfo() on single point\n",
    "geo2 = geometryTransfo(gpsShapePointsFilled[0])\n",
    "print(gpsShapePointsFilled[0]['the_geom'])\n",
    "print(geo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing gps2154() on all gpsShapePointsFilled\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gps2154PointsFilled = gps2154(gpsShapePointsFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Geo before and then 2154\n",
    "for i in range (0,10):\n",
    "    print(gpsShapePointsFilled[i]['the_geom'])\n",
    "    print(gps2154PointsFilled[i]['the_geom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for point in gps2154PointsFilled:\n",
    "    print(point)\n",
    "    print(point['the_geom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load: To PostGRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Update connection string information from env variables\n",
    "pgserver = os.getenv(\"PGSERVER\")\n",
    "pgdatabase = os.getenv(\"PGDATABASE\")\n",
    "pgusername = os.getenv(\"PGUSERNAME\")\n",
    "pgpassword = os.getenv(\"PGPWD\")\n",
    "sslmode = \"require\"\n",
    "\n",
    "# Construct connection string with psycopg2\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(pgserver, pgusername, pgdatabase, pgpassword, sslmode)\n",
    "conn = psycopg2.connect(conn_string) \n",
    "print(\"Connection established\")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open connection to PG server\n",
    "def pgOpenConnection(conn_string):\n",
    "    try:\n",
    "        conn = psycopg2.connect(conn_string)\n",
    "        print(\"Connection established\")\n",
    "        return conn\n",
    "    except psycopg2.OperationalError as err:\n",
    "        print(\"Connection could not established: \",err)\n",
    "    #return conn\n",
    "\n",
    "# Function to close connection to PG server\n",
    "def pgCloseConnection(connection):\n",
    "    try:\n",
    "        connection.close()\n",
    "        print(\"PG connection closed\")\n",
    "    except:\n",
    "        print(\"PG connection could not close successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Opening pgConnection\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(pgserver, pgusername, pgdatabase, pgpassword, sslmode)\n",
    "connection = pgOpenConnection(conn_string)\n",
    "\n",
    "# Testing Closing pgConnection\n",
    "pgCloseConnection(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label to TrashId PGSQL Data Model\n",
    "def mapLabel2TrashIdPG(label):\n",
    "    switcher = { \n",
    "        \"others\":\"1\", #\"autre dechet\" in PG Data Model mapped to IA \"others\" label\n",
    "        \"dechet agricole\":\"2\",\n",
    "        \"bottles\":\"3\", #\"bouteille boisson\" in PG Data Model mapped to IA \"bottles\" label\n",
    "        \"fragments\":\"4\",#\"industriel ou construction in PG Data Model mapped to IA \"fragments\" label\n",
    "        \"peche et chasse\":\"5\",\n",
    "        \"emballage alimentaire\":\"6\",\n",
    "        \"objet vie courante\":\"7\",\n",
    "        \"autres dechets +10\":\"8\"\n",
    "    } \n",
    "    return switcher.get(label, \"nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT REQUEST: point parameter + trashTypeId + Timestamp\n",
    "def trashInsert(gps2154Point,trashTypeId,cursor,connexion):\n",
    "    point = 'POINT(' + str(gps2154Point['the_geom'].coords.xy[0][0]) + ' ' + str(gps2154Point['the_geom'].coords.xy[1][0]) + ')'\n",
    "    elevation = gps2154Point['Elevation']\n",
    "    timestamp = gps2154Point['Time']\n",
    "    cursor.execute(\"INSERT INTO public.trash (id, id_ref_campaign_fk,the_geom, elevation, id_ref_trash_type_fk,brand_type,time ) VALUES (DEFAULT, '003d8675-29e3-4cde-a315-095ac2ec80bc',%s,%s,%s,%s,%s) RETURNING id;\", (point,elevation,trashTypeId,'icetea',timestamp))\n",
    "    connexion.commit()\n",
    "    row_id = cursor.fetchone()[0]\n",
    "    return row_id"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test: Trash INSERT using GPS point0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point0\n",
    "points = gps2154PointsFilled\n",
    "gps2154Point0 = points[0]\n",
    "\n",
    "# Trash0 from AI prediction0\n",
    "pred0 = json_prediction['detected_trash'][0]\n",
    "label0 = getTrashLabel(pred0)\n",
    "trashType0 = mapLabel2TrashIdPG(label0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT AI Trash0 with GPS Point 0 within PG\n",
    "rowID = trashInsert(gps2154Point0,trashType0,cursor,conn)\n",
    "rowID"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test: Trash INSERT using GPS dummy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Function to give GPS point to Detected Trash, based on TrashId\n",
    "def trashGPS(trashId,gps2154Points):\n",
    "    length = len(gps2154Points)+1\n",
    "    gpsIndex = trashId % length\n",
    "    return gpsIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPS coordinate for trashId0\n",
    "trashTypeId0 = pred0['id']\n",
    "gpsIndexId0 = trashGPS(trashTypeId0,gps2154PointsFilled)\n",
    "gpsTrashTypeId0 = gps2154PointsFilled[gpsIndexId0]\n",
    "\n",
    "# Trash0 from AI prediction0\n",
    "pred0 = json_prediction['detected_trash'][0]\n",
    "label0 = getTrashLabel(pred0)\n",
    "trashType0 = mapLabel2TrashIdPG(label0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT AI Trash0 with GPS from dummy trashGPS function\n",
    "rowID = trashInsert(gpsTrashTypeId0,trashType0,cursor,conn)\n",
    "rowID"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test: Trash INSERT all predictions using GPS dummy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_prediction['detected_trash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct connection string with psycopg2\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(pgserver, pgusername, pgdatabase, pgpassword, sslmode)\n",
    "conn = psycopg2.connect(conn_string) \n",
    "print(\"Connection established\")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction in json_prediction['detected_trash']:\n",
    "    # get GPS coordinate\n",
    "    trashTypeId= prediction['id']\n",
    "    gpsIndexId = trashGPS(trashTypeId,gps2154PointsFilled)\n",
    "    gpsTrashTypeId = gps2154PointsFilled[gpsIndexId]\n",
    "    # get TrashTypeId from AI prediction\n",
    "    label = getTrashLabel(prediction)\n",
    "    trashType = mapLabel2TrashIdPG(label)\n",
    "    # INSERT within PostGRE\n",
    "    trashInsert(gpsTrashTypeId,trashType,cursor,conn)"
   ]
  }
 ]
}